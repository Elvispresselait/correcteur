# üìù Plan : Mode OCR Intelligent

**Date** : 30 novembre 2024
**Objectif** : Impl√©menter un mode OCR √©conomique qui extrait le texte des captures d'√©cran et l'envoie √† l'API au lieu de l'image, avec fallback intelligent vers Vision.

---

## üéØ Objectif Final

### Cas d'usage principal
L'utilisateur capture du texte tap√© (emails, documents Word, pages web, code) et veut le faire corriger. Au lieu d'envoyer une image de 500 Ko √† GPT-4 Vision (co√ªteux), on extrait le texte localement via OCR et on envoie uniquement le texte (√©conomique et rapide).

### Flux utilisateur souhait√©

```
Utilisateur capture une zone (‚å•‚áßS)
         ‚Üì
OCR extrait le texte localement (Apple Vision)
         ‚Üì
Confiance OCR ‚â• 90% ?
    ‚îú‚îÄ‚îÄ OUI ‚Üí Envoi du texte uniquement √† l'API
    ‚îÇ         (mode √©conomique)
    ‚îÇ
    ‚îî‚îÄ‚îÄ NON ‚Üí Fallback vers Vision
              (envoi de l'image comme avant)
         ‚Üì
Affichage dans le chat :
- Image originale (miniature)
- Texte OCR extrait (si mode OCR)
- Indicateur du mode utilis√©
         ‚Üì
R√©ponse de GPT
```

---

## üìã Sp√©cifications Fonctionnelles

### 1. Mode de traitement

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| **Mode par d√©faut** | OCR (√©conomique) | R√©duire les co√ªts API |
| **Seuil de confiance** | 90% | √âquilibre entre √©conomie et qualit√© |
| **Fallback automatique** | Oui | Si confiance < 90% OU texte vide |
| **Technologie OCR** | Apple Vision (`VNRecognizeTextRequest`) | Natif, gratuit, performant |

### 2. Pr√©servation des retours √† la ligne

**Exigence critique** : Pr√©server TOUS les retours √† la ligne visuels de l'image originale.

**M√©thode** : Utiliser les bounding boxes retourn√©es par Apple Vision :
1. Trier les blocs de texte par position Y (de haut en bas)
2. D√©tecter les sauts de ligne quand l'√©cart Y entre deux blocs d√©passe un seuil
3. Reconstruire le texte avec les `\n` aux bons endroits

```
Image originale:          Texte OCR extrait:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Bonjour,        ‚îÇ  ‚Üí    "Bonjour,\n\n"
‚îÇ                 ‚îÇ       "Voici le document.\n"
‚îÇ Voici le        ‚îÇ       "Cordialement"
‚îÇ document.       ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ Cordialement    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3. Prompt unifi√©

**Modification du prompt syst√®me** : Adapter le prompt pour accepter soit une image, soit du texte OCR.

**Formulation actuelle** :
```
"Analyse l'image fournie..."
```

**Nouvelle formulation** :
```
"Analyse l'image ou le texte fourni et corrige les fautes..."
```

**Impact** : Aucun changement c√¥t√© backend, le prompt fonctionne pour les deux modes.

### 4. D√©tection automatique de la langue

| Option | Valeur |
|--------|--------|
| Langue de reconnaissance | Auto-d√©tection |
| Langues support√©es | Fran√ßais, Anglais, Espagnol, Allemand, Italien |
| Priorit√© | Fran√ßais (langue UI de l'app) |

**Code** :
```swift
request.recognitionLanguages = ["fr-FR", "en-US", "es-ES", "de-DE", "it-IT"]
request.usesLanguageCorrection = true // Am√©liore la pr√©cision
```

### 5. Affichage dans le chat

**Bulle utilisateur en mode OCR** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  [üñºÔ∏è Miniature image] [üìù OCR]              ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Texte extrait :                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                            ‚îÇ
‚îÇ  Bonjour,                                    ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Voici le document demand√©.                  ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Cordialement                                ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚ö° Mode √©conomique (confiance: 94%)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Bulle utilisateur en mode Vision (fallback)** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  [üñºÔ∏è Image compl√®te]                         ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  üîç Mode Vision (confiance OCR insuffisante) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6. Pr√©f√©rences utilisateur

**Nouvel √©l√©ment dans l'onglet "Capture"** :

```
‚îå‚îÄ Capture d'√©cran ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚òëÔ∏è Jouer un son apr√®s capture               ‚îÇ
‚îÇ  ‚òëÔ∏è Envoyer automatiquement apr√®s capture    ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îÄ‚îÄ Mode d'analyse ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ  ‚óâ Mode √©conomique (OCR)                     ‚îÇ
‚îÇ      Extrait le texte et l'envoie √† l'API.   ‚îÇ
‚îÇ      ‚ö° Plus rapide et moins cher.           ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚óã Mode Vision                               ‚îÇ
‚îÇ      Envoie l'image compl√®te √† l'API.        ‚îÇ
‚îÇ      üé® N√©cessaire pour images/graphiques.   ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚òëÔ∏è Fallback automatique si OCR incertain    ‚îÇ
‚îÇ      (confiance < 90%)                       ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 7. Gestion des erreurs

| Situation | Comportement |
|-----------|--------------|
| Texte vide (aucun texte d√©tect√©) | Fallback vers Vision + message info |
| Confiance < 90% | Fallback vers Vision automatique |
| OCR √©choue (erreur syst√®me) | Fallback vers Vision + log warning |
| Mode OCR d√©sactiv√© | Envoi image directement (comme avant) |

**Message d'erreur si OCR vide ET Vision impossible** :
```
"Aucun texte d√©tect√© dans l'image. V√©rifiez que la capture contient du texte lisible."
```

---

## üèóÔ∏è Architecture Technique

### Nouveau Service : `OCRService.swift`

```
Utilities/
‚îú‚îÄ‚îÄ ScreenCaptureService.swift    (existant)
‚îú‚îÄ‚îÄ SelectionOverlay/             (existant)
‚îî‚îÄ‚îÄ OCRService.swift              (NOUVEAU)
```

### Mod√®le de donn√©es : `OCRResult`

```swift
struct OCRResult {
    /// Texte extrait avec retours √† la ligne pr√©serv√©s
    let text: String

    /// Confiance moyenne (0.0 - 1.0)
    let confidence: Float

    /// Nombre de lignes d√©tect√©es
    let lineCount: Int

    /// Langue d√©tect√©e (code ISO)
    let detectedLanguage: String?

    /// Dur√©e de l'extraction (ms)
    let processingTimeMs: Int

    /// Indique si le fallback vers Vision est recommand√©
    var shouldFallbackToVision: Bool {
        return text.isEmpty || confidence < 0.9
    }
}
```

### Extension du mod√®le `Message`

```swift
struct Message: Identifiable, Codable {
    // ... propri√©t√©s existantes ...

    /// Mode de traitement utilis√© pour cette image (optionnel)
    var processingMode: ImageProcessingMode?

    /// R√©sultat OCR si mode OCR utilis√©
    var ocrResult: OCRResult?
}

enum ImageProcessingMode: String, Codable {
    case vision    // Image envoy√©e √† GPT-4 Vision
    case ocr       // Texte extrait localement et envoy√©
    case ocrFallback // OCR tent√© mais fallback vers Vision
}
```

### Modification de `AppPreferences`

```swift
struct AppPreferences: Codable {
    // ... existant ...

    // MARK: - Mode OCR

    /// Mode de traitement par d√©faut pour les captures
    var imageProcessingMode: ImageProcessingMode = .ocr

    /// Seuil de confiance OCR (0.0 - 1.0)
    var ocrConfidenceThreshold: Float = 0.9

    /// Activer le fallback automatique vers Vision
    var autoFallbackToVision: Bool = true
}
```

---

## üìã Plan d'Impl√©mentation D√©taill√©

> **Principe** : Chaque √©tape = 1 modification testable. On valide avant de passer √† la suite.

---

## PHASE 1 : Service OCR (fondations)

### √âTAPE 1.1 : Cr√©er le fichier `OCRService.swift` avec structure de base

**Fichier** : `Utilities/OCRService.swift`

**Objectif** : Cr√©er le squelette du service (compile, mais ne fait rien encore)

**Code** :

```swift
//
//  OCRService.swift
//  Correcteur Pro
//
//  Service d'extraction de texte (OCR) via Apple Vision
//

import Vision
import AppKit

// MARK: - OCR Service

/// Service d'extraction de texte via Apple Vision Framework
class OCRService {
    /// Singleton
    static let shared = OCRService()
    private init() {}
}
```

**Validation** :
- [ ] Le fichier est cr√©√© dans `Utilities/`
- [ ] Le projet compile sans erreur
- [ ] `OCRService.shared` est accessible

---

### √âTAPE 1.2 : Ajouter la structure `OCRResult`

**Fichier** : `Utilities/OCRService.swift`

**Objectif** : D√©finir le mod√®le de donn√©es pour les r√©sultats OCR

**Code √† ajouter** (avant `class OCRService`) :

```swift
// MARK: - OCR Result

/// R√©sultat de l'extraction OCR
struct OCRResult: Codable {
    /// Texte extrait avec retours √† la ligne pr√©serv√©s
    let text: String

    /// Confiance moyenne (0.0 - 1.0)
    let confidence: Float

    /// Nombre de blocs de texte d√©tect√©s
    let blockCount: Int

    /// Dur√©e de l'extraction (millisecondes)
    let processingTimeMs: Int

    /// Indique si le texte est vide
    var isEmpty: Bool {
        text.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty
    }

    /// Indique si le fallback vers Vision est recommand√©
    func shouldFallbackToVision(threshold: Float = 0.9) -> Bool {
        return isEmpty || confidence < threshold
    }
}
```

**Validation** :
- [ ] `OCRResult` est Codable
- [ ] `shouldFallbackToVision()` retourne `true` si confiance < 0.9

---

### √âTAPE 1.3 : Ajouter l'enum `OCRError`

**Fichier** : `Utilities/OCRService.swift`

**Objectif** : D√©finir les erreurs possibles

**Code √† ajouter** (apr√®s `OCRResult`) :

```swift
// MARK: - OCR Error

enum OCRError: LocalizedError {
    case imageConversionFailed
    case noTextFound
    case recognitionFailed(Error)

    var errorDescription: String? {
        switch self {
        case .imageConversionFailed:
            return "Impossible de convertir l'image pour l'OCR"
        case .noTextFound:
            return "Aucun texte d√©tect√© dans l'image"
        case .recognitionFailed(let error):
            return "Erreur de reconnaissance : \(error.localizedDescription)"
        }
    }
}
```

**Validation** :
- [ ] `OCRError` est LocalizedError
- [ ] Chaque cas a un message clair

---

### √âTAPE 1.4 : Impl√©menter `extractText()` (version basique)

**Fichier** : `Utilities/OCRService.swift`

**Objectif** : Extraction de texte simple (SANS pr√©servation des retours √† la ligne)

**Code √† ajouter** dans `class OCRService` :

```swift
// MARK: - Public API

/// Extrait le texte d'une image NSImage
/// - Parameter image: Image source (capture d'√©cran)
/// - Returns: OCRResult avec le texte extrait et la confiance
func extractText(from image: NSImage) async throws -> OCRResult {
    let startTime = CFAbsoluteTimeGetCurrent()

    // 1. Convertir NSImage en CGImage
    guard let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
        throw OCRError.imageConversionFailed
    }

    // 2. Cr√©er la requ√™te de reconnaissance
    let request = VNRecognizeTextRequest()
    request.recognitionLevel = .accurate
    request.usesLanguageCorrection = true
    request.recognitionLanguages = ["fr-FR", "en-US"]

    // 3. Ex√©cuter la reconnaissance
    let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

    do {
        try handler.perform([request])
    } catch {
        throw OCRError.recognitionFailed(error)
    }

    // 4. Extraire les r√©sultats
    guard let observations = request.results, !observations.isEmpty else {
        throw OCRError.noTextFound
    }

    // 5. Assembler le texte (version simple : juste concat√©ner)
    var texts: [String] = []
    var confidences: [Float] = []

    for observation in observations {
        guard let candidate = observation.topCandidates(1).first else { continue }
        texts.append(candidate.string)
        confidences.append(candidate.confidence)
    }

    let text = texts.joined(separator: "\n")
    let avgConfidence = confidences.isEmpty ? 0 : confidences.reduce(0, +) / Float(confidences.count)

    // 6. Calculer le temps de traitement
    let endTime = CFAbsoluteTimeGetCurrent()
    let processingTimeMs = Int((endTime - startTime) * 1000)

    DebugLogger.shared.log(
        "üìù OCR termin√©: \(observations.count) blocs, confiance \(Int(avgConfidence * 100))%, \(processingTimeMs)ms",
        category: "OCR"
    )

    return OCRResult(
        text: text,
        confidence: avgConfidence,
        blockCount: observations.count,
        processingTimeMs: processingTimeMs
    )
}
```

**Validation** :
- [ ] Compile sans erreur
- [ ] Test manuel : cr√©er un bouton temporaire qui appelle `OCRService.shared.extractText(from: image)` et print le r√©sultat

**Test manuel sugg√©r√©** (dans ChatView ou ailleurs, temporairement) :
```swift
Button("Test OCR") {
    Task {
        if let image = NSImage(named: "test_image") { // ou une capture
            do {
                let result = try await OCRService.shared.extractText(from: image)
                print("OCR: \(result.text)")
                print("Confiance: \(result.confidence)")
            } catch {
                print("Erreur OCR: \(error)")
            }
        }
    }
}
```

---

### √âTAPE 1.5 : Am√©liorer avec pr√©servation des retours √† la ligne

**Fichier** : `Utilities/OCRService.swift`

**Objectif** : Trier les blocs par position Y pour pr√©server la structure du texte

**Modification** : Remplacer la section "5. Assembler le texte" par :

```swift
// 5. Reconstruire le texte avec pr√©servation des retours √† la ligne
let (text, avgConfidence) = reconstructTextWithLineBreaks(
    observations: observations,
    imageHeight: CGFloat(cgImage.height)
)
```

**Ajouter cette m√©thode priv√©e** dans `class OCRService` :

```swift
// MARK: - Private Methods

/// Reconstruit le texte en pr√©servant les retours √† la ligne visuels
private func reconstructTextWithLineBreaks(
    observations: [VNRecognizedTextObservation],
    imageHeight: CGFloat
) -> (text: String, confidence: Float) {

    // Trier par position Y (de haut en bas)
    // Note: Les coordonn√©es Vision sont normalis√©es (0-1) avec Y invers√© (0 = bas)
    let sortedObservations = observations.sorted { obs1, obs2 in
        // Y plus grand = plus haut dans l'image
        obs1.boundingBox.origin.y > obs2.boundingBox.origin.y
    }

    var lines: [String] = []
    var confidences: [Float] = []
    var lastY: CGFloat = 1.0 // Commence en haut

    // Seuil pour d√©tecter un saut de paragraphe (en proportion de la hauteur)
    let paragraphThreshold: CGFloat = 0.05 // 5% de la hauteur = nouveau paragraphe

    for observation in sortedObservations {
        guard let candidate = observation.topCandidates(1).first else { continue }

        let currentY = observation.boundingBox.origin.y + observation.boundingBox.height
        let yDifference = lastY - currentY

        // D√©tecter si on doit ajouter une ligne vide (nouveau paragraphe)
        if yDifference > paragraphThreshold && !lines.isEmpty {
            lines.append("") // Ligne vide = s√©paration de paragraphe
        }

        lines.append(candidate.string)
        confidences.append(candidate.confidence)
        lastY = observation.boundingBox.origin.y
    }

    let text = lines.joined(separator: "\n")
    let avgConfidence = confidences.isEmpty ? 0 : confidences.reduce(0, +) / Float(confidences.count)

    return (text, avgConfidence)
}
```

**Validation** :
- [ ] Compile sans erreur
- [ ] Test avec une image contenant plusieurs paragraphes ‚Üí les sauts de ligne sont pr√©serv√©s

---

### √âTAPE 1.6 : Test complet du service OCR en isolation

**Objectif** : Valider que le service fonctionne avant de l'int√©grer

**Tests √† effectuer** :

| Test | Image | R√©sultat attendu |
|------|-------|------------------|
| Texte simple | Email avec 1 paragraphe | Texte extrait, confiance > 90% |
| Multi-paragraphes | Document avec espaces | Retours √† la ligne pr√©serv√©s |
| Texte flou | Screenshot basse qualit√© | Confiance < 90%, `shouldFallbackToVision() = true` |
| Image sans texte | Photo de paysage | Exception `OCRError.noTextFound` |
| Image vide | Rectangle blanc | Exception `OCRError.noTextFound` |

**Validation finale √©tape 1** :
- [ ] Tous les tests passent
- [ ] Le service est pr√™t pour l'int√©gration

---

## PHASE 2 : Mod√®les de donn√©es

### √âTAPE 2.1 : Ajouter `ImageProcessingMode` dans AppPreferences

**Fichier** : `Models/AppPreferences.swift`

**Objectif** : Cr√©er l'enum pour le mode de traitement

**Code √† ajouter** (en haut du fichier, avant `struct AppPreferences`) :

```swift
// MARK: - Image Processing Mode

/// Mode de traitement pour les captures d'√©cran
enum ImageProcessingMode: String, Codable, CaseIterable {
    case ocr = "ocr"           // Extraction texte puis envoi
    case vision = "vision"     // Envoi image directement

    var displayName: String {
        switch self {
        case .ocr: return "Mode √©conomique (OCR)"
        case .vision: return "Mode Vision"
        }
    }

    var description: String {
        switch self {
        case .ocr: return "Extrait le texte et l'envoie √† l'API. Plus rapide et moins cher."
        case .vision: return "Envoie l'image compl√®te √† l'API. N√©cessaire pour images/graphiques."
        }
    }

    var icon: String {
        switch self {
        case .ocr: return "bolt.fill"
        case .vision: return "eye.fill"
        }
    }
}
```

**Validation** :
- [ ] Compile sans erreur
- [ ] `ImageProcessingMode.allCases` retourne [.ocr, .vision]

---

### √âTAPE 2.2 : Ajouter les pr√©f√©rences OCR dans AppPreferences

**Fichier** : `Models/AppPreferences.swift`

**Objectif** : Ajouter les propri√©t√©s de configuration OCR

**Code √† ajouter** dans `struct AppPreferences` :

```swift
// MARK: - Mode OCR

/// Mode de traitement par d√©faut pour les captures
var imageProcessingMode: ImageProcessingMode = .ocr

/// Seuil de confiance OCR (0.0 - 1.0) - fallback vers Vision si inf√©rieur
var ocrConfidenceThreshold: Float = 0.9

/// Activer le fallback automatique vers Vision si OCR incertain
var autoFallbackToVision: Bool = true
```

**Validation** :
- [ ] Compile sans erreur
- [ ] `PreferencesManager.shared.preferences.imageProcessingMode` retourne `.ocr` par d√©faut

---

### √âTAPE 2.3 : Ajouter les m√©tadonn√©es OCR dans Message

**Fichier** : `Models/Message.swift`

**Objectif** : Stocker les infos OCR dans chaque message

**Code √† ajouter** dans `struct Message` :

```swift
// MARK: - OCR Metadata

/// Mode de traitement utilis√© pour les images de ce message
var imageProcessingMode: ImageProcessingMode?

/// Texte OCR extrait (si mode OCR)
var ocrText: String?

/// Confiance OCR (si mode OCR)
var ocrConfidence: Float?

/// Indique si un fallback vers Vision a √©t√© effectu√©
var didFallbackToVision: Bool = false
```

**Validation** :
- [ ] Compile sans erreur
- [ ] Les anciens messages chargent toujours (propri√©t√©s optionnelles)
- [ ] Cr√©er un nouveau message ‚Üí les propri√©t√©s OCR sont nil par d√©faut

---

## PHASE 3 : Int√©gration dans le ViewModel

### √âTAPE 3.1 : Ajouter la m√©thode de traitement OCR (sans l'utiliser)

**Fichier** : `ViewModels/ChatViewModel.swift`

**Objectif** : Cr√©er la logique OCR sans modifier le flux existant

**Code √† ajouter** (nouvelle section MARK) :

```swift
// MARK: - OCR Processing

/// R√©sultat du traitement d'une image
struct ImageProcessingResult {
    let ocrText: String?        // Texte extrait (nil si Vision)
    let sendImage: Bool         // true = envoyer l'image
    let ocrResult: OCRResult?   // R√©sultat OCR complet
    let fallbackReason: String? // Raison du fallback (nil si pas de fallback)
    let mode: ImageProcessingMode
}

/// Traite une image selon le mode configur√©
/// - Parameter image: Image √† traiter
/// - Returns: R√©sultat avec d√©cision OCR/Vision
private func processImageForSending(_ image: NSImage) async -> ImageProcessingResult {
    let preferences = PreferencesManager.shared.preferences

    // Si mode Vision forc√©, envoyer directement l'image
    guard preferences.imageProcessingMode == .ocr else {
        DebugLogger.shared.log("üé® Mode Vision: envoi image directement", category: "OCR")
        return ImageProcessingResult(
            ocrText: nil,
            sendImage: true,
            ocrResult: nil,
            fallbackReason: nil,
            mode: .vision
        )
    }

    // Tenter l'OCR
    do {
        let ocrResult = try await OCRService.shared.extractText(from: image)

        DebugLogger.shared.logCapture(
            "üìù OCR: \(ocrResult.blockCount) blocs, confiance \(Int(ocrResult.confidence * 100))%"
        )

        // V√©rifier si fallback n√©cessaire
        if ocrResult.shouldFallbackToVision(threshold: preferences.ocrConfidenceThreshold) {
            if preferences.autoFallbackToVision {
                let reason = ocrResult.isEmpty
                    ? "Aucun texte d√©tect√©"
                    : "Confiance insuffisante (\(Int(ocrResult.confidence * 100))%)"

                DebugLogger.shared.logWarning("‚ö†Ô∏è Fallback Vision: \(reason)")
                return ImageProcessingResult(
                    ocrText: nil,
                    sendImage: true,
                    ocrResult: ocrResult,
                    fallbackReason: reason,
                    mode: .vision
                )
            }
        }

        // OCR r√©ussi
        return ImageProcessingResult(
            ocrText: ocrResult.text,
            sendImage: false,
            ocrResult: ocrResult,
            fallbackReason: nil,
            mode: .ocr
        )

    } catch {
        // Erreur OCR ‚Üí fallback vers Vision
        DebugLogger.shared.logError("‚ùå OCR √©chou√©: \(error.localizedDescription)")

        return ImageProcessingResult(
            ocrText: nil,
            sendImage: true,
            ocrResult: nil,
            fallbackReason: "Erreur: \(error.localizedDescription)",
            mode: .vision
        )
    }
}
```

**Validation** :
- [ ] Compile sans erreur
- [ ] La m√©thode existe mais n'est pas encore appel√©e
- [ ] Le flux existant fonctionne toujours

---

### √âTAPE 3.2 : Cr√©er une m√©thode `sendMessageWithOCR` (parall√®le √† l'existante)

**Fichier** : `ViewModels/ChatViewModel.swift`

**Objectif** : Nouvelle m√©thode qui int√®gre l'OCR, sans casser l'existante

**Code √† ajouter** :

```swift
/// Envoie un message avec traitement OCR des images
/// - Parameters:
///   - text: Texte du message
///   - images: Images √† traiter
/// - Returns: true si envoi r√©ussi
@MainActor
func sendMessageWithOCR(_ text: String, images: [NSImage]) async -> Bool {
    guard !images.isEmpty else {
        // Pas d'images, utiliser sendMessage classique
        return sendMessage(text, images: [])
    }

    // Traiter chaque image
    var processedText = text
    var imagesToSend: [NSImage] = []
    var ocrTexts: [String] = []
    var finalMode: ImageProcessingMode = .ocr
    var finalConfidence: Float = 0
    var didFallback = false

    for image in images {
        let result = await processImageForSending(image)

        if let ocrText = result.ocrText {
            ocrTexts.append(ocrText)
            finalConfidence = result.ocrResult?.confidence ?? 0
        }

        if result.sendImage {
            imagesToSend.append(image)
            finalMode = .vision
            if result.fallbackReason != nil {
                didFallback = true
            }
        }
    }

    // Construire le message final
    if !ocrTexts.isEmpty {
        let ocrContent = ocrTexts.joined(separator: "\n\n---\n\n")
        if processedText.isEmpty {
            processedText = ocrContent
        } else {
            processedText = "\(processedText)\n\n---\n\nTexte extrait :\n\(ocrContent)"
        }
    }

    // Envoyer avec ou sans images selon le mode
    let success = sendMessage(processedText, images: imagesToSend)

    // TODO: Stocker les m√©tadonn√©es OCR dans le message (√©tape suivante)

    return success
}
```

**Validation** :
- [ ] Compile sans erreur
- [ ] Test manuel avec une image : v√©rifier les logs OCR
- [ ] Le message est envoy√© correctement

---

### √âTAPE 3.3 : Connecter `sendMessageWithOCR` aux captures d'√©cran

**Fichier** : `Views/ContentView.swift`

**Objectif** : Utiliser la nouvelle m√©thode pour les captures

**Modification** dans `handleCapturedImage()` :

```swift
/// Traite une image captur√©e re√ßue via notification
private func handleCapturedImage(_ image: NSImage) {
    // Auto-envoi si activ√© ET conversation s√©lectionn√©e
    if PreferencesManager.shared.preferences.autoSendOnCapture,
       viewModel.selectedConversationID != nil {

        // Utiliser sendMessageWithOCR pour le traitement intelligent
        Task {
            _ = await viewModel.sendMessageWithOCR("", images: [image])
            DebugLogger.shared.logCapture("‚úÖ Capture envoy√©e (mode OCR)")

            // Forcer le scroll vers le bas
            await MainActor.run {
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                    NotificationCenter.default.post(name: .forceScrollToBottom, object: nil)
                }
            }
        }
    } else {
        viewModel.capturedImage = image
        DebugLogger.shared.logCapture("‚úÖ Capture ajout√©e en attente")
    }
}
```

**Validation** :
- [ ] Capture ‚å•‚áßS ‚Üí l'OCR est appel√© (v√©rifier les logs)
- [ ] Si confiance > 90% ‚Üí texte envoy√© (pas d'image)
- [ ] Si confiance < 90% ‚Üí image envoy√©e (fallback)

---

### √âTAPE 3.4 : Stocker les m√©tadonn√©es OCR dans le Message

**Fichier** : `ViewModels/ChatViewModel.swift`

**Objectif** : Sauvegarder le mode utilis√© dans le message

**Modification** : Cette √©tape n√©cessite de modifier `sendMessage()` pour accepter les m√©tadonn√©es OCR, OU de modifier le message apr√®s envoi.

**Option simple** : Ajouter une propri√©t√© `@Published` temporaire et la lire apr√®s envoi :

```swift
// Dans ChatViewModel, ajouter :
@Published var lastOCRResult: (mode: ImageProcessingMode, confidence: Float, didFallback: Bool)?

// Dans sendMessageWithOCR, avant return :
lastOCRResult = (finalMode, finalConfidence, didFallback)

// Puis modifier le dernier message ajout√© pour y stocker les infos
if success, let lastMessage = conversations[safe: currentIndex]?.messages.last {
    // Mettre √† jour le message avec les infos OCR
    // (n√©cessite de rendre Message mutable ou d'utiliser un index)
}
```

**Note** : Cette √©tape est plus complexe et peut √™tre simplifi√©e. On peut la reporter √† la Phase 4.

**Validation** :
- [ ] Le mode utilis√© est visible dans les logs
- [ ] (Optionnel) Le message contient les m√©tadonn√©es

---

## PHASE 4 : Interface utilisateur

### √âTAPE 4.1 : Ajouter le toggle OCR/Vision dans les pr√©f√©rences

**Fichier** : `Views/Preferences/CapturePreferencesView.swift`

**Objectif** : UI pour choisir le mode

**Code √† ajouter** (nouvelle Section) :

```swift
// MARK: - Mode d'analyse

Section {
    VStack(alignment: .leading, spacing: 12) {
        Text("Mode d'analyse des captures")
            .font(.headline)

        // Picker pour le mode
        Picker("Mode", selection: $prefsManager.preferences.imageProcessingMode) {
            ForEach(ImageProcessingMode.allCases, id: \.self) { mode in
                HStack {
                    Image(systemName: mode.icon)
                    Text(mode.displayName)
                }
                .tag(mode)
            }
        }
        .pickerStyle(.radioGroup)
        .onChange(of: prefsManager.preferences.imageProcessingMode) { _, _ in
            prefsManager.save()
        }

        Text(prefsManager.preferences.imageProcessingMode.description)
            .font(.caption)
            .foregroundColor(.secondary)
    }
} header: {
    Text("Traitement des images")
}
```

**Validation** :
- [ ] Le toggle est visible dans Pr√©f√©rences > Capture
- [ ] Changer le mode sauvegarde la pr√©f√©rence
- [ ] Au red√©marrage, le mode est restaur√©

---

### √âTAPE 4.2 : Ajouter l'option de fallback automatique

**Fichier** : `Views/Preferences/CapturePreferencesView.swift`

**Objectif** : Toggle pour activer/d√©sactiver le fallback

**Code √† ajouter** (dans la m√™me Section) :

```swift
// Visible seulement si mode OCR
if prefsManager.preferences.imageProcessingMode == .ocr {
    Divider()

    Toggle("Fallback automatique vers Vision", isOn: $prefsManager.preferences.autoFallbackToVision)
        .onChange(of: prefsManager.preferences.autoFallbackToVision) { _, _ in
            prefsManager.save()
        }

    Text("Si la confiance OCR est inf√©rieure √† 90%, l'image sera envoy√©e √† la place du texte.")
        .font(.caption)
        .foregroundColor(.secondary)
}
```

**Validation** :
- [ ] L'option appara√Æt seulement en mode OCR
- [ ] Le toggle fonctionne et sauvegarde

---

### √âTAPE 4.3 : Afficher l'indicateur de mode dans les messages

**Fichier** : `Views/ChatView.swift` (dans MessageBubble)

**Objectif** : Montrer quel mode a √©t√© utilis√©

**Code √† ajouter** (en bas de la bulle utilisateur, si message a des images) :

```swift
// Indicateur de mode (si applicable)
if let mode = message.imageProcessingMode {
    HStack(spacing: 4) {
        Image(systemName: mode.icon)
            .font(.caption2)

        Text(mode == .ocr ? "Mode √©conomique" : "Mode Vision")
            .font(.caption2)

        if message.didFallbackToVision {
            Text("(fallback)")
                .font(.caption2)
                .foregroundColor(.orange)
        }
    }
    .foregroundColor(mode == .ocr ? .green : .blue)
    .padding(.top, 4)
}
```

**Validation** :
- [ ] L'indicateur s'affiche sur les nouveaux messages avec images
- [ ] Vert pour OCR, bleu pour Vision
- [ ] "fallback" en orange si applicable

---

### √âTAPE 4.4 : Afficher le texte OCR extrait dans la bulle

**Fichier** : `Views/ChatView.swift` (dans MessageBubble)

**Objectif** : Montrer le texte extrait √† l'utilisateur

**Code √† ajouter** (apr√®s l'image, si ocrText pr√©sent) :

```swift
// Texte OCR extrait
if let ocrText = message.ocrText, !ocrText.isEmpty {
    VStack(alignment: .leading, spacing: 6) {
        Divider()
            .background(Color.white.opacity(0.2))

        Text("Texte extrait :")
            .font(.caption)
            .fontWeight(.medium)
            .foregroundColor(.white.opacity(0.6))

        Text(ocrText)
            .font(.body)
            .foregroundColor(.white.opacity(0.9))
            .textSelection(.enabled)
            .padding(8)
            .background(
                RoundedRectangle(cornerRadius: 6)
                    .fill(Color.white.opacity(0.05))
            )
    }
}
```

**Validation** :
- [ ] Le texte OCR s'affiche sous l'image
- [ ] Le texte est s√©lectionnable
- [ ] Le style est coh√©rent avec le reste de l'UI

---

## PHASE 5 : Finalisation

### √âTAPE 5.1 : Adapter le prompt syst√®me

**Fichier** : `Models/AppPreferences.swift`

**Objectif** : Rendre le prompt compatible avec texte OU image

**Modification** : Dans `defaultPromptCorrecteur`, remplacer :
- "l'image" ‚Üí "l'image ou le texte"
- "Analyse l'image" ‚Üí "Analyse le contenu"

**Exemple** :
```swift
// Avant :
"Analyse l'image fournie et corrige les fautes d'orthographe..."

// Apr√®s :
"Analyse l'image ou le texte fourni et corrige les fautes d'orthographe..."
```

**Validation** :
- [ ] Le prompt fonctionne avec une image (comme avant)
- [ ] Le prompt fonctionne avec du texte OCR

---

### √âTAPE 5.2 : Tests complets

**Objectif** : Valider tous les sc√©narios

| # | Test | Mode | Attendu |
|---|------|------|---------|
| 1 | Capture texte clair | OCR | Texte extrait, envoy√©, indicateur vert |
| 2 | Capture texte flou | OCR‚ÜíVision | Fallback, image envoy√©e, indicateur "fallback" |
| 3 | Capture graphique | OCR‚ÜíVision | Fallback (pas de texte) |
| 4 | Mode Vision forc√© | Vision | Image envoy√©e, indicateur bleu |
| 5 | Fallback d√©sactiv√© + OCR incertain | OCR | Texte envoy√© quand m√™me |
| 6 | Pr√©f√©rences persist√©es | - | Mode et options sauvegard√©s au red√©marrage |
| 7 | Anciens messages | - | Pas de crash, pas d'indicateur OCR |

**Validation finale** :
- [ ] Tous les tests passent
- [ ] Pas de r√©gression sur l'existant
- [ ] Performance acceptable (< 500ms pour OCR)

---

### √âTAPE 5.3 : Documentation et commit

**Objectif** : Finaliser et documenter

1. Mettre √† jour `CLAUDE.md` avec les nouvelles infos OCR
2. Commit avec message descriptif
3. Tester une derni√®re fois en production

---

## üìä R√©sum√© des √©tapes

| Phase | √âtape | Description | Complexit√© |
|-------|-------|-------------|------------|
| 1 | 1.1 | Cr√©er OCRService (squelette) | üü¢ Simple |
| 1 | 1.2 | Ajouter OCRResult | üü¢ Simple |
| 1 | 1.3 | Ajouter OCRError | üü¢ Simple |
| 1 | 1.4 | Impl√©menter extractText (basique) | üü° Moyen |
| 1 | 1.5 | Pr√©servation retours √† la ligne | üü° Moyen |
| 1 | 1.6 | Tests service OCR | üü¢ Simple |
| 2 | 2.1 | Ajouter ImageProcessingMode | üü¢ Simple |
| 2 | 2.2 | Ajouter pr√©f√©rences OCR | üü¢ Simple |
| 2 | 2.3 | Ajouter m√©tadonn√©es Message | üü¢ Simple |
| 3 | 3.1 | M√©thode processImageForSending | üü° Moyen |
| 3 | 3.2 | M√©thode sendMessageWithOCR | üü° Moyen |
| 3 | 3.3 | Connecter aux captures | üü¢ Simple |
| 3 | 3.4 | Stocker m√©tadonn√©es | üü† Complexe |
| 4 | 4.1 | Toggle mode dans pr√©f√©rences | üü¢ Simple |
| 4 | 4.2 | Option fallback | üü¢ Simple |
| 4 | 4.3 | Indicateur mode dans bulle | üü¢ Simple |
| 4 | 4.4 | Afficher texte OCR | üü¢ Simple |
| 5 | 5.1 | Adapter prompt | üü¢ Simple |
| 5 | 5.2 | Tests complets | üü° Moyen |
| 5 | 5.3 | Documentation | üü¢ Simple |

**Total : 20 micro-√©tapes**

---

## ‚ö†Ô∏è Points de validation critiques

Apr√®s chaque phase, s'assurer que :

- **Fin Phase 1** : OCRService fonctionne en isolation (test manuel)
- **Fin Phase 2** : Les mod√®les compilent et sont r√©trocompatibles
- **Fin Phase 3** : Les captures utilisent l'OCR (v√©rifier logs)
- **Fin Phase 4** : L'UI refl√®te le mode utilis√©
- **Fin Phase 5** : Tout fonctionne de bout en bout

---

**Ancienne version du code complet** (pour r√©f√©rence) :

```swift
//
//  OCRService.swift
//  Correcteur Pro
//
//  Service d'extraction de texte (OCR) via Apple Vision
//

import Vision
import AppKit

// MARK: - OCR Result

/// R√©sultat de l'extraction OCR
struct OCRResult: Codable {
    /// Texte extrait avec retours √† la ligne pr√©serv√©s
    let text: String

    /// Confiance moyenne (0.0 - 1.0)
    let confidence: Float

    /// Nombre de blocs de texte d√©tect√©s
    let blockCount: Int

    /// Langues d√©tect√©es
    let detectedLanguages: [String]

    /// Dur√©e de l'extraction (millisecondes)
    let processingTimeMs: Int

    /// Indique si le texte est vide
    var isEmpty: Bool { text.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty }

    /// Indique si le fallback vers Vision est recommand√©
    func shouldFallbackToVision(threshold: Float = 0.9) -> Bool {
        return isEmpty || confidence < threshold
    }
}

// MARK: - OCR Error

enum OCRError: LocalizedError {
    case imageConversionFailed
    case noTextFound
    case recognitionFailed(Error)
    case cancelled

    var errorDescription: String? {
        switch self {
        case .imageConversionFailed:
            return "Impossible de convertir l'image pour l'OCR"
        case .noTextFound:
            return "Aucun texte d√©tect√© dans l'image"
        case .recognitionFailed(let error):
            return "Erreur de reconnaissance : \(error.localizedDescription)"
        case .cancelled:
            return "Reconnaissance annul√©e"
        }
    }
}

// MARK: - OCR Service

/// Service d'extraction de texte via Apple Vision Framework
class OCRService {

    /// Singleton
    static let shared = OCRService()
    private init() {}

    // MARK: - Public API

    /// Extrait le texte d'une image NSImage
    /// - Parameters:
    ///   - image: Image source (capture d'√©cran)
    ///   - languages: Langues de reconnaissance (d√©faut: fr, en)
    /// - Returns: OCRResult avec le texte extrait et la confiance
    func extractText(from image: NSImage, languages: [String]? = nil) async throws -> OCRResult {
        let startTime = CFAbsoluteTimeGetCurrent()

        // 1. Convertir NSImage en CGImage
        guard let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
            throw OCRError.imageConversionFailed
        }

        // 2. Cr√©er la requ√™te de reconnaissance
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate // Pr√©cision maximale
        request.usesLanguageCorrection = true // Correction linguistique

        // Langues de reconnaissance (priorit√© : fran√ßais, anglais, puis autres)
        let recognitionLanguages = languages ?? ["fr-FR", "en-US", "es-ES", "de-DE", "it-IT"]
        request.recognitionLanguages = recognitionLanguages

        // 3. Ex√©cuter la reconnaissance
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

        do {
            try handler.perform([request])
        } catch {
            throw OCRError.recognitionFailed(error)
        }

        // 4. Extraire les r√©sultats
        guard let observations = request.results, !observations.isEmpty else {
            throw OCRError.noTextFound
        }

        // 5. Reconstruire le texte avec pr√©servation des retours √† la ligne
        let (text, avgConfidence) = reconstructTextWithLineBreaks(
            observations: observations,
            imageHeight: CGFloat(cgImage.height)
        )

        // 6. D√©tecter les langues pr√©sentes
        let detectedLanguages = detectLanguages(in: text)

        // 7. Calculer le temps de traitement
        let endTime = CFAbsoluteTimeGetCurrent()
        let processingTimeMs = Int((endTime - startTime) * 1000)

        // 8. Logger le r√©sultat
        DebugLogger.shared.log(
            "üìù OCR termin√©: \(observations.count) blocs, confiance \(Int(avgConfidence * 100))%, \(processingTimeMs)ms",
            category: "OCR"
        )

        return OCRResult(
            text: text,
            confidence: avgConfidence,
            blockCount: observations.count,
            detectedLanguages: detectedLanguages,
            processingTimeMs: processingTimeMs
        )
    }

    // MARK: - Private Methods

    /// Reconstruit le texte en pr√©servant les retours √† la ligne visuels
    private func reconstructTextWithLineBreaks(
        observations: [VNRecognizedTextObservation],
        imageHeight: CGFloat
    ) -> (text: String, confidence: Float) {

        // Trier par position Y (de haut en bas)
        // Note: Les coordonn√©es Vision sont normalis√©es (0-1) avec Y invers√©
        let sortedObservations = observations.sorted { obs1, obs2 in
            // Y plus grand = plus haut dans l'image (Vision inverse Y)
            obs1.boundingBox.origin.y > obs2.boundingBox.origin.y
        }

        var lines: [String] = []
        var confidences: [Float] = []
        var lastY: CGFloat = 1.0 // Commence en haut

        // Seuil pour d√©tecter un saut de ligne (en proportion de la hauteur)
        // Une ligne de texte standard fait environ 2-3% de la hauteur de l'image
        let lineHeightThreshold: CGFloat = 0.025

        for observation in sortedObservations {
            guard let candidate = observation.topCandidates(1).first else { continue }

            let currentY = observation.boundingBox.origin.y + observation.boundingBox.height
            let yDifference = lastY - currentY

            // D√©tecter si on doit ajouter un retour √† la ligne
            if yDifference > lineHeightThreshold * 2 {
                // Grand √©cart = paragraphe (ajouter ligne vide)
                if !lines.isEmpty {
                    lines.append("")
                }
            }

            lines.append(candidate.string)
            confidences.append(candidate.confidence)
            lastY = observation.boundingBox.origin.y
        }

        let text = lines.joined(separator: "\n")
        let avgConfidence = confidences.isEmpty ? 0 : confidences.reduce(0, +) / Float(confidences.count)

        return (text, avgConfidence)
    }

    /// D√©tecte les langues pr√©sentes dans le texte
    private func detectLanguages(in text: String) -> [String] {
        let recognizer = NLLanguageRecognizer()
        recognizer.processString(text)

        var languages: [String] = []

        if let dominantLanguage = recognizer.dominantLanguage {
            languages.append(dominantLanguage.rawValue)
        }

        // Ajouter les autres langues d√©tect√©es avec probabilit√© > 20%
        let hypotheses = recognizer.languageHypotheses(withMaximum: 3)
        for (language, probability) in hypotheses where probability > 0.2 {
            if !languages.contains(language.rawValue) {
                languages.append(language.rawValue)
            }
        }

        return languages
    }
}
```

**V√©rification** :
- [ ] Le fichier compile sans erreur
- [ ] Les imports Vision et NaturalLanguage sont pr√©sents
- [ ] La structure OCRResult est bien Codable

**Risques** :
- Performance avec tr√®s grandes images ‚Üí Tester avec captures 4K
- Ordre des blocs incorrect ‚Üí Ajuster `lineHeightThreshold`

---

### √âTAPE 2 : Modifier `AppPreferences.swift`

**Fichier** : `Models/AppPreferences.swift`

**Objectif** : Ajouter les pr√©f√©rences pour le mode OCR

**Modifications** :

```swift
// Ajouter apr√®s les pr√©f√©rences de capture existantes

// MARK: - Mode OCR

/// Mode de traitement pour les captures d'√©cran
enum ImageProcessingMode: String, Codable, CaseIterable {
    case ocr = "ocr"           // Extraction texte puis envoi
    case vision = "vision"     // Envoi image directement

    var displayName: String {
        switch self {
        case .ocr: return "Mode √©conomique (OCR)"
        case .vision: return "Mode Vision"
        }
    }

    var description: String {
        switch self {
        case .ocr: return "Extrait le texte et l'envoie √† l'API. Plus rapide et moins cher."
        case .vision: return "Envoie l'image compl√®te √† l'API. N√©cessaire pour images/graphiques."
        }
    }

    var icon: String {
        switch self {
        case .ocr: return "‚ö°"
        case .vision: return "üé®"
        }
    }
}

// Dans struct AppPreferences, ajouter :

/// Mode de traitement par d√©faut pour les captures
var imageProcessingMode: ImageProcessingMode = .ocr

/// Seuil de confiance OCR (0.0 - 1.0) - fallback vers Vision si inf√©rieur
var ocrConfidenceThreshold: Float = 0.9

/// Activer le fallback automatique vers Vision si OCR incertain
var autoFallbackToVision: Bool = true
```

**V√©rification** :
- [ ] `ImageProcessingMode` est Codable et CaseIterable
- [ ] Valeurs par d√©faut correctes (OCR par d√©faut, seuil 0.9)

---

### √âTAPE 3 : Modifier `Message.swift`

**Fichier** : `Models/Message.swift`

**Objectif** : Stocker le mode de traitement et les infos OCR dans chaque message

**Modifications** :

```swift
// Ajouter dans struct Message :

/// Mode de traitement utilis√© pour les images de ce message
var imageProcessingMode: ImageProcessingMode?

/// Texte OCR extrait (si mode OCR)
var ocrText: String?

/// Confiance OCR (si mode OCR)
var ocrConfidence: Float?

/// Indique si un fallback vers Vision a √©t√© effectu√©
var didFallbackToVision: Bool = false
```

**Note** : Ces propri√©t√©s sont optionnelles pour rester compatible avec les messages existants.

**V√©rification** :
- [ ] Les nouvelles propri√©t√©s sont optionnelles
- [ ] La r√©trocompatibilit√© avec les anciens messages est pr√©serv√©e

---

### √âTAPE 4 : Modifier `ChatViewModel.swift`

**Fichier** : `ViewModels/ChatViewModel.swift`

**Objectif** : Int√©grer la logique OCR dans le flux d'envoi de message

**Modifications principales** :

```swift
// MARK: - OCR Processing

/// Traite une image selon le mode configur√©
/// - Returns: (messageText, shouldSendImage, ocrResult)
private func processImageForSending(_ image: NSImage) async -> (
    text: String?,
    sendImage: Bool,
    ocrResult: OCRResult?,
    fallbackReason: String?
) {
    let preferences = PreferencesManager.shared.preferences

    // Si mode Vision, envoyer directement l'image
    guard preferences.imageProcessingMode == .ocr else {
        DebugLogger.shared.log("üé® Mode Vision: envoi image directement", category: "OCR")
        return (nil, true, nil, nil)
    }

    // Tenter l'OCR
    do {
        let ocrResult = try await OCRService.shared.extractText(from: image)

        DebugLogger.shared.logCapture(
            "üìù OCR: \(ocrResult.blockCount) blocs, confiance \(Int(ocrResult.confidence * 100))%"
        )

        // V√©rifier si fallback n√©cessaire
        if ocrResult.shouldFallbackToVision(threshold: preferences.ocrConfidenceThreshold) {
            if preferences.autoFallbackToVision {
                let reason = ocrResult.isEmpty
                    ? "Aucun texte d√©tect√©"
                    : "Confiance insuffisante (\(Int(ocrResult.confidence * 100))%)"

                DebugLogger.shared.logWarning("‚ö†Ô∏è Fallback Vision: \(reason)")
                return (nil, true, ocrResult, reason)
            } else {
                // Pas de fallback auto, envoyer le texte OCR quand m√™me
                DebugLogger.shared.logWarning("‚ö†Ô∏è OCR incertain mais fallback d√©sactiv√©")
                return (ocrResult.text, false, ocrResult, nil)
            }
        }

        // OCR r√©ussi avec bonne confiance
        return (ocrResult.text, false, ocrResult, nil)

    } catch {
        // Erreur OCR ‚Üí fallback vers Vision
        DebugLogger.shared.logError("‚ùå OCR √©chou√©: \(error.localizedDescription)")

        if preferences.autoFallbackToVision {
            return (nil, true, nil, "Erreur OCR: \(error.localizedDescription)")
        } else {
            // Pas de fallback, annuler l'envoi
            return (nil, false, nil, "Erreur OCR: \(error.localizedDescription)")
        }
    }
}

// Modifier sendMessage() pour int√©grer l'OCR :

func sendMessage(_ text: String, images: [NSImage] = []) -> Bool {
    // ... code existant pour validation ...

    // Traitement OCR si images pr√©sentes
    var processedImages: [NSImage] = []
    var ocrTexts: [String] = []
    var ocrResults: [OCRResult] = []
    var fallbackReasons: [String] = []

    if !images.isEmpty {
        Task {
            for image in images {
                let result = await processImageForSending(image)

                if let ocrText = result.text {
                    ocrTexts.append(ocrText)
                }
                if result.sendImage {
                    processedImages.append(image)
                }
                if let ocrResult = result.ocrResult {
                    ocrResults.append(ocrResult)
                }
                if let reason = result.fallbackReason {
                    fallbackReasons.append(reason)
                }
            }

            // Continuer l'envoi avec les donn√©es trait√©es
            await MainActor.run {
                self.sendProcessedMessage(
                    text: text,
                    ocrTexts: ocrTexts,
                    images: processedImages,
                    ocrResults: ocrResults,
                    fallbackReasons: fallbackReasons
                )
            }
        }
        return true
    }

    // Pas d'images, envoi normal
    // ... code existant ...
}
```

**V√©rification** :
- [ ] La logique OCR est correctement int√©gr√©e
- [ ] Le fallback vers Vision fonctionne
- [ ] Les m√©tadonn√©es OCR sont sauvegard√©es dans le Message

---

### √âTAPE 5 : Modifier la vue de pr√©f√©rences

**Fichier** : `Views/Preferences/CapturePreferencesView.swift`

**Objectif** : Ajouter les options OCR dans l'interface de pr√©f√©rences

**Code** :

```swift
// MARK: - Mode d'analyse

Section {
    VStack(alignment: .leading, spacing: 12) {
        Text("Mode d'analyse")
            .font(.headline)
            .foregroundColor(.white)

        // Radio buttons pour le mode
        ForEach(ImageProcessingMode.allCases, id: \.self) { mode in
            HStack(alignment: .top, spacing: 10) {
                Image(systemName: preferences.imageProcessingMode == mode
                    ? "circle.inset.filled"
                    : "circle")
                    .foregroundColor(preferences.imageProcessingMode == mode
                        ? .blue
                        : .gray)
                    .onTapGesture {
                        preferences.imageProcessingMode = mode
                    }

                VStack(alignment: .leading, spacing: 4) {
                    Text("\(mode.icon) \(mode.displayName)")
                        .font(.body)
                        .foregroundColor(.white)

                    Text(mode.description)
                        .font(.caption)
                        .foregroundColor(.gray)
                }
            }
            .padding(.vertical, 4)
        }

        // Option fallback (visible seulement si mode OCR)
        if preferences.imageProcessingMode == .ocr {
            Divider()
                .background(Color.white.opacity(0.1))

            Toggle(isOn: $preferences.autoFallbackToVision) {
                VStack(alignment: .leading, spacing: 2) {
                    Text("Fallback automatique si OCR incertain")
                        .font(.body)
                        .foregroundColor(.white)

                    Text("Envoie l'image si la confiance OCR < \(Int(preferences.ocrConfidenceThreshold * 100))%")
                        .font(.caption)
                        .foregroundColor(.gray)
                }
            }
            .toggleStyle(SwitchToggleStyle(tint: .blue))
        }
    }
} header: {
    Text("Traitement des captures")
}
```

**V√©rification** :
- [ ] Les radio buttons fonctionnent
- [ ] L'option fallback est visible seulement en mode OCR
- [ ] Les pr√©f√©rences sont sauvegard√©es correctement

---

### √âTAPE 6 : Modifier l'affichage des messages

**Fichier** : `Views/ChatView.swift` (MessageBubble)

**Objectif** : Afficher le texte OCR et l'indicateur de mode dans la bulle utilisateur

**Modifications** :

```swift
// Dans MessageBubble, ajouter apr√®s l'affichage de l'image :

// Affichage du texte OCR extrait
if let ocrText = message.ocrText, !ocrText.isEmpty {
    VStack(alignment: .leading, spacing: 8) {
        // S√©parateur
        Rectangle()
            .fill(Color.white.opacity(0.1))
            .frame(height: 1)

        // Label "Texte extrait"
        Text("Texte extrait :")
            .font(.caption)
            .fontWeight(.medium)
            .foregroundColor(.white.opacity(0.6))

        // Texte OCR
        Text(ocrText)
            .font(.body)
            .foregroundColor(.white.opacity(0.9))
            .textSelection(.enabled)
            .padding(10)
            .background(
                RoundedRectangle(cornerRadius: 8)
                    .fill(Color.white.opacity(0.05))
            )
    }
}

// Indicateur de mode (en bas de la bulle)
if message.imageProcessingMode != nil {
    HStack(spacing: 6) {
        if message.imageProcessingMode == .ocr {
            Image(systemName: "bolt.fill")
                .font(.caption2)
                .foregroundColor(.green)

            if let confidence = message.ocrConfidence {
                Text("Mode √©conomique (\(Int(confidence * 100))%)")
                    .font(.caption2)
                    .foregroundColor(.green.opacity(0.8))
            }
        } else if message.didFallbackToVision {
            Image(systemName: "eye.fill")
                .font(.caption2)
                .foregroundColor(.orange)

            Text("Vision (fallback)")
                .font(.caption2)
                .foregroundColor(.orange.opacity(0.8))
        } else {
            Image(systemName: "eye.fill")
                .font(.caption2)
                .foregroundColor(.blue)

            Text("Mode Vision")
                .font(.caption2)
                .foregroundColor(.blue.opacity(0.8))
        }
    }
    .padding(.top, 6)
}
```

**V√©rification** :
- [ ] Le texte OCR s'affiche correctement
- [ ] L'indicateur de mode est visible
- [ ] Les couleurs sont coh√©rentes (vert = OCR, orange = fallback, bleu = Vision)

---

### √âTAPE 7 : Modifier le prompt syst√®me

**Fichier** : `Models/AppPreferences.swift` (defaultPromptCorrecteur)

**Objectif** : Adapter le prompt pour accepter image OU texte

**Modification** :

Remplacer toute r√©f√©rence √† "l'image" par "l'image ou le texte".

**Exemple** :

```swift
// Avant :
"Analyse l'image fournie et corrige les fautes..."

// Apr√®s :
"Analyse l'image ou le texte fourni et corrige les fautes..."
```

**Note** : Cette modification est r√©trocompatible car le prompt fonctionne d√©j√† pour du texte pur.

---

### √âTAPE 8 : Tests et validation

**Tests manuels √† effectuer** :

| Test | Mode | R√©sultat attendu |
|------|------|------------------|
| Capture texte clair (email) | OCR | Texte extrait, envoy√© comme texte |
| Capture texte flou | OCR | Fallback vers Vision (confiance < 90%) |
| Capture image/graphique | OCR | Fallback vers Vision (pas de texte) |
| Capture texte avec mode Vision forc√© | Vision | Image envoy√©e directement |
| Fallback d√©sactiv√© + OCR incertain | OCR | Texte OCR envoy√© (m√™me si incertain) |
| Capture texte multilingue | OCR | D√©tection langue correcte |
| Tr√®s longue capture (document) | OCR | Performance acceptable (< 2s) |

**Tests de r√©gression** :

| Test | R√©sultat attendu |
|------|------------------|
| Envoi message texte seul | Fonctionne comme avant |
| Envoi image sans OCR configur√© | Fonctionne comme avant |
| Changement de conversation | Pas de crash |
| Rechargement app | Pr√©f√©rences OCR persist√©es |

---

## üîß Points d'attention techniques

### 1. Performance OCR

**Probl√®me potentiel** : L'OCR peut √™tre lent sur des images tr√®s grandes.

**Solutions** :
- Ex√©cuter l'OCR en arri√®re-plan (async/await)
- Afficher un indicateur de progression "Extraction du texte..."
- Redimensionner l'image si trop grande (> 4K)

### 2. Pr√©servation des retours √† la ligne

**Probl√®me potentiel** : Apple Vision retourne les blocs dans un ordre impr√©visible.

**Solution impl√©ment√©e** :
- Tri par coordonn√©e Y (boundingBox)
- D√©tection d'√©cart Y pour ins√©rer des `\n`
- Seuil configurable (`lineHeightThreshold`)

### 3. R√©trocompatibilit√©

**Exigence** : Les anciens messages (sans metadata OCR) doivent continuer √† fonctionner.

**Solution** :
- Toutes les nouvelles propri√©t√©s sont optionnelles
- Codable avec decoder configur√© pour valeurs manquantes
- UI s'adapte (n'affiche pas l'indicateur OCR si nil)

### 4. Co√ªt API

**Estimation des √©conomies** :

| Mode | Co√ªt par requ√™te (approx.) |
|------|----------------------------|
| Vision (image 500 Ko) | ~0.01$ (selon taille) |
| OCR (texte 2000 chars) | ~0.0002$ |
| **√âconomie** | **~98%** |

---

## üìä M√©triques de succ√®s

### Fonctionnel
- [ ] OCR extrait correctement le texte de captures d'√©cran standard
- [ ] Les retours √† la ligne sont pr√©serv√©s fid√®lement
- [ ] Le fallback vers Vision fonctionne automatiquement
- [ ] L'indicateur de mode est visible et correct

### Performance
- [ ] OCR < 500ms pour image standard (1080p)
- [ ] OCR < 2s pour tr√®s grande image (4K)
- [ ] Pas de freeze de l'UI pendant l'OCR

### UX
- [ ] Le toggle OCR/Vision est clair et accessible
- [ ] L'utilisateur comprend quel mode est utilis√©
- [ ] Pas de r√©gression sur le workflow existant

---

## üìÅ Fichiers √† cr√©er/modifier

| Fichier | Action | Priorit√© |
|---------|--------|----------|
| `Utilities/OCRService.swift` | CR√âER | P0 |
| `Models/AppPreferences.swift` | MODIFIER | P0 |
| `Models/Message.swift` | MODIFIER | P0 |
| `ViewModels/ChatViewModel.swift` | MODIFIER | P0 |
| `Views/Preferences/CapturePreferencesView.swift` | MODIFIER | P1 |
| `Views/ChatView.swift` (MessageBubble) | MODIFIER | P1 |

---

## üöÄ Ordre d'impl√©mentation recommand√©

1. **Phase 1 - Core (P0)**
   - √âtape 1 : Cr√©er `OCRService.swift`
   - √âtape 2 : Modifier `AppPreferences.swift`
   - √âtape 3 : Modifier `Message.swift`
   - Test : V√©rifier que l'OCR fonctionne en isolation

2. **Phase 2 - Int√©gration (P0)**
   - √âtape 4 : Modifier `ChatViewModel.swift`
   - Test : V√©rifier le flux complet (capture ‚Üí OCR ‚Üí envoi)

3. **Phase 3 - UI (P1)**
   - √âtape 5 : Modifier les pr√©f√©rences
   - √âtape 6 : Modifier l'affichage des messages
   - √âtape 7 : Adapter le prompt
   - Test : V√©rifier l'UX compl√®te

4. **Phase 4 - Polish**
   - √âtape 8 : Tests complets
   - Ajustements selon retours
   - Documentation mise √† jour

---

## ‚ö†Ô∏è Risques et mitigations

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| OCR impr√©cis sur certaines polices | Moyenne | Moyen | Fallback automatique vers Vision |
| Performance d√©grad√©e sur 4K | Faible | Faible | Redimensionnement pr√©alable |
| Retours √† la ligne mal d√©tect√©s | Moyenne | Moyen | Ajuster `lineHeightThreshold` |
| Anciens messages cass√©s | Faible | √âlev√© | Propri√©t√©s optionnelles + tests |
| Pr√©f√©rences non sauvegard√©es | Faible | Moyen | V√©rifier `PreferencesManager` |

---

## üìù Notes finales

Ce plan d√©taille une impl√©mentation progressive du mode OCR intelligent. Les phases P0 doivent √™tre compl√©t√©es en premier car elles constituent le c≈ìur de la fonctionnalit√©. Les phases P1 am√©liorent l'UX mais ne sont pas bloquantes pour les tests initiaux.

**Points cl√©s √† retenir** :
1. **OCR par d√©faut** = √©conomie de co√ªts significative
2. **Fallback automatique** = pas de perte de qualit√©
3. **Pr√©servation des retours √† la ligne** = fid√©lit√© au document original
4. **Indicateur visible** = transparence pour l'utilisateur

La prochaine √©tape apr√®s validation de ce plan est de commencer l'impl√©mentation par `OCRService.swift`.
