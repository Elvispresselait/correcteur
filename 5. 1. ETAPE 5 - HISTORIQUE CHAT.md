# ğŸ“‹ Plan d'action : Mode conversation avec historique

## ğŸ¯ Objectif
AmÃ©liorer `OpenAIService` pour supporter l'historique conversationnel, permettant Ã  ChatGPT de se souvenir du contexte prÃ©cÃ©dent dans une conversation.

---

## ğŸ” ProblÃ¨me Ã  rÃ©soudre
- Actuellement, chaque message est envoyÃ© isolÃ©ment Ã  l'API
- ChatGPT ne peut pas se souvenir des messages prÃ©cÃ©dents
- Pas de contexte conversationnel maintenu
- Impossible d'avoir des conversations cohÃ©rentes sur plusieurs Ã©changes

---

## âœ… Solution proposÃ©e
Modifier `OpenAIService` et `ChatViewModel` pour :
1. Envoyer tout l'historique de la conversation Ã  l'API
2. Limiter l'historique aux 20 derniers messages pour Ã©conomiser les tokens
3. Afficher le nombre de tokens estimÃ©s
4. Ajouter des optimisations (debounce, annulation, retry)

---

## ğŸš€ PLAN D'ACTION EN 4 Ã‰TAPES

### ğŸ“ Ã‰TAPE 5.1 : Modifier OpenAIService pour accepter l'historique
**Objectif** : Adapter `OpenAIService` pour envoyer tout l'historique conversationnel

**Actions** :
1. Modifier la signature de `sendMessage` dans `OpenAIService.swift` :
   ```swift
   // Ancienne signature :
   static func sendMessage(message: String, systemPrompt: String) async throws -> String
   
   // Nouvelle signature :
   static func sendMessage(messages: [Message], systemPrompt: String) async throws -> String
   ```

2. CrÃ©er une mÃ©thode de conversion `convertMessagesToOpenAIFormat` :
   ```swift
   private static func convertMessagesToOpenAIFormat(
       _ messages: [Message],
       systemPrompt: String
   ) -> [[String: Any]] {
       var openAIMessages: [[String: Any]] = []
       
       // 1. Ajouter le systemPrompt en premier
       openAIMessages.append([
           "role": "system",
           "content": systemPrompt
       ])
       
       // 2. Convertir les messages utilisateur et assistant
       for message in messages {
           let role = message.isUser ? "user" : "assistant"
           openAIMessages.append([
               "role": role,
               "content": message.contenu
           ])
       }
       
       return openAIMessages
   }
   ```

3. Modifier le body JSON de la requÃªte :
   ```swift
   let requestBody: [String: Any] = [
       "model": model,
       "messages": convertMessagesToOpenAIFormat(messages, systemPrompt: systemPrompt),
       "temperature": 0.7,
       "max_tokens": 2000
   ]
   ```

4. Garder la mÃ©thode ancienne pour compatibilitÃ© (optionnel) :
   ```swift
   // MÃ©thode de compatibilitÃ© (dÃ©prÃ©ciÃ©e)
   static func sendMessage(message: String, systemPrompt: String) async throws -> String {
       let singleMessage = Message(contenu: message, isUser: true)
       return try await sendMessage(messages: [singleMessage], systemPrompt: systemPrompt)
   }
   ```

**Fichiers Ã  modifier** :
- `Correcteur Pro/Services/OpenAIService.swift`

**Validation** : `OpenAIService.sendMessage()` accepte maintenant un tableau de messages et envoie tout l'historique Ã  l'API.

---

### ğŸ“ Ã‰TAPE 5.2 : Modifier ChatViewModel pour passer l'historique
**Objectif** : Passer toute la conversation active Ã  l'API au lieu d'un seul message

**Actions** :
1. Modifier `ChatViewModel.sendMessage()` :
   ```swift
   // RÃ©cupÃ©rer tous les messages de la conversation actuelle
   let conversationMessages = conversations[index].messages
   
   // Limiter aux 20 derniers messages pour Ã©conomiser les tokens
   let recentMessages = Array(conversationMessages.suffix(20))
   
   // Appeler OpenAIService avec tout l'historique
   let response = try await OpenAIService.sendMessage(
       messages: recentMessages,
       systemPrompt: systemPrompt
   )
   ```

2. GÃ©rer le cas oÃ¹ il n'y a pas encore de messages :
   - Si c'est le premier message, envoyer seulement le message utilisateur
   - Le systemPrompt sera toujours inclus en premier

3. Exclure le message temporaire (typing indicator) de l'historique :
   ```swift
   // Filtrer les messages temporaires avant d'envoyer
   let messagesToSend = conversationMessages.filter { message in
       !message.contenu.contains("â³ GÃ©nÃ©ration en cours...")
   }
   ```

4. Logs pour debug :
   ```swift
   print("ğŸ“ [ChatViewModel] Envoi de \(messagesToSend.count) message(s) Ã  l'API")
   print("ğŸ“ [ChatViewModel] Messages: \(messagesToSend.map { $0.isUser ? "User" : "Assistant" })")
   ```

**Fichiers Ã  modifier** :
- `Correcteur Pro/ViewModels/ChatViewModel.swift`

**Validation** : Quand on envoie un message, tout l'historique de la conversation est envoyÃ© Ã  l'API.

---

### ğŸ“ Ã‰TAPE 5.3 : Gestion du contexte et affichage des tokens
**Objectif** : Afficher le nombre de tokens estimÃ©s et avertir si la conversation est trop longue

**Actions** :
1. CrÃ©er une fonction d'estimation des tokens :
   ```swift
   // Approximation : 4 caractÃ¨res = 1 token (rÃ¨gle gÃ©nÃ©rale OpenAI)
   private func estimateTokens(for messages: [Message], systemPrompt: String) -> Int {
       let totalChars = messages.reduce(0) { $0 + $1.contenu.count } + systemPrompt.count
       return totalChars / 4
   }
   ```

2. Ajouter un affichage dans le header :
   - Afficher le nombre de tokens estimÃ©s Ã  cÃ´tÃ© du titre de la conversation
   - Format : "Conversation (â‰ˆ 450 tokens)"
   - Couleur : blanc avec opacitÃ© 0.6

3. Afficher un warning si > 3000 tokens :
   - Banner jaune/orange dans le header
   - Message : "âš ï¸ Conversation longue (â‰ˆ X tokens). Les rÃ©ponses peuvent Ãªtre plus lentes."
   - Bouton "Nouvelle conversation" pour reset

4. Ajouter un compteur de messages :
   - Afficher "X messages" dans le header
   - Mettre Ã  jour automatiquement

**Fichiers Ã  modifier** :
- `Correcteur Pro/ViewModels/ChatViewModel.swift` : Fonction d'estimation
- `Correcteur Pro/Views/ChatView.swift` : Affichage dans HeaderView

**Validation** : Le nombre de tokens estimÃ©s s'affiche dans le header et un warning apparaÃ®t si > 3000 tokens.

---

### ğŸ“ Ã‰TAPE 5.4 : Optimisations (debounce, annulation, retry)
**Objectif** : AmÃ©liorer l'expÃ©rience utilisateur avec des optimisations

**Actions** :
1. **Debounce** : EmpÃªcher l'envoi de multiples messages simultanÃ©s
   - DÃ©jÃ  gÃ©rÃ© par `isGenerating`, mais amÃ©liorer :
   ```swift
   // Dans ChatViewModel
   private var sendTask: Task<Void, Never>?
   
   func sendMessage(...) {
       // Annuler la tÃ¢che prÃ©cÃ©dente si elle existe
       sendTask?.cancel()
       
       // CrÃ©er une nouvelle tÃ¢che
       sendTask = Task {
           // ... code d'envoi
       }
   }
   ```

2. **Bouton "Stop"** : Annuler une gÃ©nÃ©ration en cours
   - Ajouter un bouton "Stop" Ã  cÃ´tÃ© du bouton d'envoi pendant `isGenerating`
   - Action : annuler le `Task` en cours
   - Remplacer le message temporaire par "âŒ GÃ©nÃ©ration annulÃ©e"

3. **Bouton "Retry"** : RegÃ©nÃ©rer la derniÃ¨re rÃ©ponse
   - Ajouter un bouton "Retry" sur le dernier message assistant en cas d'erreur
   - Action : supprimer le dernier message assistant et le dernier message user, puis renvoyer
   - Ou : renvoyer seulement le dernier message user

4. **Gestion de l'annulation** :
   ```swift
   // Dans le Task
   do {
       let response = try await OpenAIService.sendMessage(...)
       // VÃ©rifier si la tÃ¢che a Ã©tÃ© annulÃ©e
       try Task.checkCancellation()
       // ... continuer
   } catch is CancellationError {
       print("âš ï¸ [ChatViewModel] GÃ©nÃ©ration annulÃ©e par l'utilisateur")
       // Remplacer le message temporaire
   }
   ```

**Fichiers Ã  modifier** :
- `Correcteur Pro/ViewModels/ChatViewModel.swift` : Gestion des tÃ¢ches et annulation
- `Correcteur Pro/Views/ChatView.swift` : Boutons Stop et Retry

**Validation** : On peut annuler une gÃ©nÃ©ration en cours, et regÃ©nÃ©rer une rÃ©ponse en cas d'erreur.

---

## ğŸ¯ Ordre d'implÃ©mentation recommandÃ©

1. **Ã‰TAPE 5.1** (OpenAIService) - 30 min
   - Modifier la signature et la conversion des messages
   - Tester avec un historique simple

2. **Ã‰TAPE 5.2** (ChatViewModel) - 20 min
   - Passer tout l'historique Ã  l'API
   - Limiter aux 20 derniers messages

3. **Ã‰TAPE 5.3** (Affichage tokens) - 25 min
   - Estimation des tokens
   - Affichage dans le header
   - Warning si > 3000 tokens

4. **Ã‰TAPE 5.4** (Optimisations) - 30 min
   - Debounce et annulation
   - Boutons Stop et Retry

**Total estimÃ©** : ~1h45

---

## ğŸ”§ Fichiers Ã  crÃ©er/modifier

### Fichiers Ã  modifier :
- `Correcteur Pro/Services/OpenAIService.swift` : Nouvelle signature avec historique
- `Correcteur Pro/ViewModels/ChatViewModel.swift` : Passer l'historique, estimation tokens
- `Correcteur Pro/Views/ChatView.swift` : Affichage tokens, boutons Stop/Retry

---

## âœ… CritÃ¨res de validation finale

L'Ã‰TAPE 5 est validÃ©e si :
- âœ… On peut envoyer plusieurs messages dans une conversation
- âœ… ChatGPT se souvient des messages prÃ©cÃ©dents
- âœ… Le nombre de tokens estimÃ©s s'affiche dans le header
- âœ… Un warning s'affiche si > 3000 tokens
- âœ… On peut annuler une gÃ©nÃ©ration en cours
- âœ… On peut regÃ©nÃ©rer une rÃ©ponse en cas d'erreur
- âœ… Les nouvelles conversations repartent de zÃ©ro (pas d'historique)
- âœ… L'historique est limitÃ© aux 20 derniers messages

---

## ğŸ› Gestion des erreurs

### Cas Ã  gÃ©rer :
- **Historique vide** : Envoyer seulement le message utilisateur + systemPrompt
- **Historique trop long** : Limiter aux 20 derniers messages
- **Annulation** : Nettoyer proprement le message temporaire
- **Erreur rÃ©seau pendant l'envoi** : Permettre le retry

### Gestion recommandÃ©e :
- Logger le nombre de messages envoyÃ©s
- Afficher un message si l'historique est tronquÃ© (> 20 messages)
- GÃ©rer proprement l'annulation (pas de crash)

---

## ğŸ“š Ressources

- [OpenAI Chat Completions API - Messages](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)
- [OpenAI Token Counting](https://platform.openai.com/tokenizer)
- [Swift Task Cancellation](https://developer.apple.com/documentation/swift/task)

---

## ğŸ”„ Prochaines Ã©tapes (aprÃ¨s Ã‰TAPE 5)

Une fois l'historique conversationnel terminÃ©, passer Ã  :
- **Ã‰TAPE 6** : Support des images dans l'API (Vision)
- **Ã‰TAPE 7** : Persistance et sauvegarde des conversations

---

## ğŸ“ Notes de dÃ©veloppement

- **Estimation tokens** : Approximation (4 chars = 1 token), pas exact mais suffisant pour l'UI
- **Limite 20 messages** : Ã‰quilibre entre contexte et coÃ»t (environ 2000-3000 tokens)
- **SystemPrompt** : Toujours inclus en premier, ne compte pas dans la limite de 20 messages
- **Performance** : L'envoi de l'historique peut ralentir lÃ©gÃ¨rement, mais nÃ©cessaire pour le contexte
- **Debug** : Logger le nombre de messages et tokens estimÃ©s pour faciliter le debugging

